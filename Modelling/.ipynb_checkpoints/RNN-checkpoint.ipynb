{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline for Pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error,  r2_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "#import tensorflow.keras.layers as layers\n",
    "from scipy.sparse import isspmatrix\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "#from scikeras.wrappers import KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "\n",
    "# Train Data \n",
    "temp_df = pd.read_csv(\"../../data/cleaned/train.csv\", nrows=0)  # Read only the header\n",
    "total_columns = len(temp_df.columns)\n",
    "columns_to_use = temp_df.columns[1:total_columns] \n",
    "train_data = pd.read_csv(\"../../data/cleaned/train.csv\", usecols=columns_to_use)\n",
    "\n",
    "# Test Data \n",
    "test_data = pd.read_csv(\"../../data/cleaned/test.csv\", usecols=columns_to_use)\n",
    "\n",
    "# Dropping the columns that are not relevant to our analysis \n",
    "train_data = train_data.drop(columns=['building_name', 'site_name'])\n",
    "test_data = test_data.drop(columns=['building_name', 'site_name'])\n",
    "\n",
    "# Building index on building_id for furhter assessment \n",
    "#train_data.set_index('building_id', inplace=True)\n",
    "#test_data.set_index('building_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering for electrity meter_reading\n",
    "train_data = train_data[train_data['meter'] == 'electricity']\n",
    "test_data = test_data[test_data['meter'] == 'electricity']\n",
    "\n",
    "train_data = train_data.drop(columns=['meter'])\n",
    "test_data = test_data.drop(columns=['meter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              date  meter_reading sub_primaryspaceusage      sqm      sqft  \\\n",
      "34688   2016-10-11          0.000              Academic   6052.1   65144.0   \n",
      "258138  2016-04-18      12691.183    College Laboratory  14537.7  156483.0   \n",
      "\n",
      "           timezone  airTemperature  cloudCoverage  dewTemperature  \\\n",
      "34688   US/Mountain       14.455044       1.334908        7.430044   \n",
      "258138   US/Central       15.188158       1.747599        4.782018   \n",
      "\n",
      "        precipDepth1HR  precipDepth6HR  seaLvlPressure  windDirection  \\\n",
      "34688         0.366489       11.150400     1022.003142     145.886540   \n",
      "258138        0.993188       12.707306     1020.930271     169.464298   \n",
      "\n",
      "        windSpeed  season  building_id  site_id  \n",
      "34688    2.772494    Fall           76        2  \n",
      "258138   3.517544  Spring          309        8  \n",
      "-------------------------------------------------------------\n",
      "              date  meter_reading sub_primaryspaceusage      sqm      sqft  \\\n",
      "232419  2017-10-07      2873.2800    College Laboratory   8339.9   89770.0   \n",
      "2539    2017-12-16     12866.7721             Education  13142.2  141461.0   \n",
      "\n",
      "           timezone  airTemperature  cloudCoverage  dewTemperature  \\\n",
      "232419  US/Mountain       18.858991       1.728504       12.960249   \n",
      "2539     US/Pacific        5.395098       1.839549       -0.425980   \n",
      "\n",
      "        precipDepth1HR  precipDepth6HR  seaLvlPressure  windDirection  \\\n",
      "232419        0.744561       11.555278     1013.606398     184.813872   \n",
      "2539          0.614097       11.242905     1017.455537     206.292266   \n",
      "\n",
      "        windSpeed  season  building_id  site_id  \n",
      "232419   3.741667    Fall          280        7  \n",
      "2539     3.361520  Winter            7        1  \n"
     ]
    }
   ],
   "source": [
    "# Inspecting the data frames\n",
    "print(train_data.sample(2))\n",
    "print('-------------------------------------------------------------')\n",
    "print(test_data.sample(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating into X and Y dataframes \n",
    "X_train = train_data.drop(columns=['meter_reading'])  # Exclude target variable\n",
    "y_train = train_data['meter_reading']\n",
    "\n",
    "X_test = test_data.drop(columns=['meter_reading'])  # Exclude target variable\n",
    "y_test = test_data['meter_reading']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up a pipeline to process the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'site_id' from numeric to categorical \n",
    "X_train['site_id'] = X_train['site_id'].astype('category')\n",
    "X_test['site_id'] = X_test['site_id'].astype('category')\n",
    "\n",
    "# Making sure the date columns is in the right format \n",
    "X_train['date'] = pd.to_datetime(X_train['date'])\n",
    "X_test['date'] = pd.to_datetime(X_test['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date                     datetime64[ns]\n",
      "sub_primaryspaceusage            object\n",
      "sqm                             float64\n",
      "sqft                            float64\n",
      "timezone                         object\n",
      "airTemperature                  float64\n",
      "cloudCoverage                   float64\n",
      "dewTemperature                  float64\n",
      "precipDepth1HR                  float64\n",
      "precipDepth6HR                  float64\n",
      "seaLvlPressure                  float64\n",
      "windDirection                   float64\n",
      "windSpeed                       float64\n",
      "season                           object\n",
      "building_id                       int64\n",
      "site_id                        category\n",
      "dtype: object\n",
      "Index(['date', 'sub_primaryspaceusage', 'sqm', 'sqft', 'timezone',\n",
      "       'airTemperature', 'cloudCoverage', 'dewTemperature', 'precipDepth1HR',\n",
      "       'precipDepth6HR', 'seaLvlPressure', 'windDirection', 'windSpeed',\n",
      "       'season', 'building_id', 'site_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(X_train.dtypes)\n",
    "print(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and types based on your dataset\n",
    "numerical_features = ['sqm', 'sqft', 'airTemperature', 'cloudCoverage', 'dewTemperature',\n",
    "                      'precipDepth1HR', 'precipDepth6HR', 'seaLvlPressure', 'windDirection', 'windSpeed']\n",
    "categorical_features = ['timezone', 'season', 'sub_primaryspaceusage', 'site_id']\n",
    "date_feature = 'date'\n",
    "id_feature = 'building_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the 'building_id' and 'date' columns\n",
    "building_ids_train = X_train[id_feature].values\n",
    "dates_train = X_train[date_feature].values\n",
    "building_ids_test = X_test[id_feature].values\n",
    "dates_test = X_test[date_feature].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'building_id' and 'date' columns for preprocessing\n",
    "X_train = X_train.drop(columns=[id_feature, date_feature])\n",
    "X_test = X_test.drop(columns=[id_feature, date_feature])\n",
    "\n",
    "# Create a preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', MinMaxScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Fit the preprocessor on the training data and transform both training and test data\n",
    "preprocessor.fit(X_train)\n",
    "X_train_processed = preprocessor.transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the processed data back to dense DataFrames\n",
    "X_train_processed_df = pd.DataFrame(X_train_processed.toarray(), columns=preprocessor.get_feature_names_out())\n",
    "X_test_processed_df = pd.DataFrame(X_test_processed.toarray(), columns=preprocessor.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reattach the 'building_id' and 'date' columns to the processed data\n",
    "X_train_processed_df[id_feature] = building_ids_train\n",
    "X_train_processed_df[date_feature] = dates_train\n",
    "X_test_processed_df[id_feature] = building_ids_test\n",
    "X_test_processed_df[date_feature] = dates_test\n",
    "\n",
    "# Sort the DataFrames by 'building_id' and 'date' to ensure the correct sequence\n",
    "X_train_processed_df.sort_values(by=[id_feature, date_feature], inplace=True)\n",
    "X_test_processed_df.sort_values(by=[id_feature, date_feature], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['num__sqm', 'num__sqft', 'num__airTemperature', 'num__cloudCoverage',\n",
       "       'num__dewTemperature', 'num__precipDepth1HR', 'num__precipDepth6HR',\n",
       "       'num__seaLvlPressure', 'num__windDirection', 'num__windSpeed',\n",
       "       'cat__timezone_Europe/Dublin', 'cat__timezone_Europe/London',\n",
       "       'cat__timezone_US/Central', 'cat__timezone_US/Eastern',\n",
       "       'cat__timezone_US/Mountain', 'cat__timezone_US/Pacific',\n",
       "       'cat__season_Fall', 'cat__season_Spring', 'cat__season_Summer',\n",
       "       'cat__season_Winter', 'cat__sub_primaryspaceusage_Academic',\n",
       "       'cat__sub_primaryspaceusage_Auditorium',\n",
       "       'cat__sub_primaryspaceusage_Classroom',\n",
       "       'cat__sub_primaryspaceusage_College Classroom',\n",
       "       'cat__sub_primaryspaceusage_College Laboratory',\n",
       "       'cat__sub_primaryspaceusage_Education',\n",
       "       'cat__sub_primaryspaceusage_K-12 School',\n",
       "       'cat__sub_primaryspaceusage_Other - Education',\n",
       "       'cat__sub_primaryspaceusage_Primary/Secondary Classroom',\n",
       "       'cat__sub_primaryspaceusage_Research',\n",
       "       'cat__sub_primaryspaceusage_School',\n",
       "       'cat__sub_primaryspaceusage_Student Center',\n",
       "       'cat__sub_primaryspaceusage_Student Union', 'cat__site_id_1',\n",
       "       'cat__site_id_2', 'cat__site_id_3', 'cat__site_id_4', 'cat__site_id_5',\n",
       "       'cat__site_id_6', 'cat__site_id_7', 'cat__site_id_8', 'cat__site_id_9',\n",
       "       'cat__site_id_10', 'cat__site_id_11', 'cat__site_id_12',\n",
       "       'cat__site_id_13', 'cat__site_id_14', 'cat__site_id_15', 'building_id',\n",
       "       'date'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_processed_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume y_train and y_test are Pandas Series with 'meter_reading' for training and testing datasets\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit scaler on the training set\n",
    "y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
    "\n",
    "# Only transform the test set, do not fit the scaler to it to avoid data leakage\n",
    "y_test_scaled = scaler.transform(y_test.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making it suitable for timeseries data \n",
    "sequence_length = 5\n",
    "\n",
    "def create_sequences(data, target, sequence_length):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(sequence_length, data.shape[0]):  # Start from sequence_length\n",
    "        if isspmatrix(data):  # Check if 'data' is a sparse matrix\n",
    "            seq = data[(i - sequence_length):i].toarray()  # Convert to dense array\n",
    "        else:  # If 'data' is already a dense array or DataFrame\n",
    "            seq = data.iloc[(i - sequence_length):i].values if hasattr(data, 'iloc') else data[(i - sequence_length):i]\n",
    "        X.append(seq)\n",
    "        y.append(target[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Ensure y_train_scaled and y_test_scaled are numpy arrays for consistency\n",
    "y_train_scaled = np.array(y_train_scaled).flatten()\n",
    "y_test_scaled = np.array(y_test_scaled).flatten()\n",
    "\n",
    "# Create sequences using the modified function\n",
    "X_train_sequences, y_train_scaled_sequences = create_sequences(X_train_processed, y_train_scaled, sequence_length)\n",
    "X_test_sequences, y_test_scaled_sequences = create_sequences(X_test_processed, y_test_scaled, sequence_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 10:53:01.139891: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2024-03-05 10:53:01.139952: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2024-03-05 10:53:01.139963: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2024-03-05 10:53:01.140443: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-03-05 10:53:01.140992: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "# Define RNN model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(X_train_sequences.shape[1], X_train_sequences.shape[2])))\n",
    "model.add(Dense(1))  # The output layer with one neuron, as we are doing regression to predict 'meter reading'\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Fit the RNN model on the training data\n",
    "history = model.fit(X_train_sequences, y_train_scaled, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "y_pred_scaled = model.predict(X_test_sequences)\n",
    "y_pred_scaled = np.nan_to_num(y_pred_scaled, nan=0)\n",
    "y_test_scaled = y_test_scaled[sequence_length:]\n",
    "\n",
    "# Inverse the scaling of the predictions to get them on the same scale as the original 'meter reading' data\n",
    "y_pred = scaler.inverse_transform(y_pred_scaled)\n",
    "y_true = scaler.inverse_transform(y_test_scaled.reshape(-1, 1))  # Reshape if y_test_scaled is a 1D array\n",
    "\n",
    "# Calculate MSE and R^2\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "r_squared = r2_score(y_true, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error (MSE) on Test Data: {mse}')\n",
    "print(f'R-squared Score on Test Data: {r_squared}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting y_pred against y_true\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_true, y_pred, color='blue', alpha=0.5)\n",
    "plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'k--', lw=2)  # Plotting the diagonal line\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('True vs Predicted Values')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def create_rnn_model(lstm_units=50, activation='relu', optimizer='adam', input_shape=(None, None)):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=lstm_units, activation=activation, input_shape=input_shape))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model\n",
    "\n",
    "# Note: Adjust the input_shape dynamically based on your actual data\n",
    "#model = KerasRegressor(model=create_rnn_model, lstm_units=50, optimizer='adam', input_shape=(X_train_sequences.shape[1], X_train_sequences.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'model__lstm_units': [20, 50, 100],\n",
    "    'model__activation': ['relu', 'tanh', 'sigmoid'],  # Including different activation functions to try\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'epochs': [10, 20],\n",
    "    'optimizer': ['adam', 'rmsprop']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasRegressor(model=create_rnn_model, input_shape=(X_train_sequences.shape[1], X_train_sequences.shape[2]))\n",
    "\n",
    "# Setup GridSearchCV with the updated param_grid including 'model__activation'\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Perform the search\n",
    "grid_result = grid.fit(X_train_sequences, y_train_scaled)\n",
    "\n",
    "# Summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
